// This file is computer-generated by onnx2c 
// (TODO: add creating command line here)
// (TODO: print creation date here )

// ONNX model:
// produced by pytorch, version 2.6.0
// ONNX IR version: 18
// Model documentation: 
/*

*/

#include <float.h>
#include <math.h>
#include <stdbool.h>
#include <stdint.h>
#include <string.h>
#include <stdlib.h>

#include "network.h"

#define MAX(X,Y) ( X > Y ? X : Y)
#define MIN(X,Y) ( X < Y ? X : Y)
#define CLIP(X,L) ( MAX(MIN(X,L), -L) )

#if __STDC_VERSION__ < 199901L
#define FUNC_PREFIX
#else
#define FUNC_PREFIX static inline
#endif

static const float tensor_onnx__Conv_78[8][8][1][1] = 
{
  {
    {
      {0.19776517152786254883f}
    },
    {
      {-0.14232346415519714355f}
    },
    {
      {-0.34352189302444458008f}
    },
    {
      {0.12557990849018096924f}
    },
    {
      {-0.21643295884132385254f}
    },
    {
      {0.14548744261264801025f}
    },
    {
      {-0.09045439213514328003f}
    },
    {
      {0.31741330027580261230f}
    }
  },
  {
    {
      {-0.04521676525473594666f}
    },
    {
      {-0.20264764130115509033f}
    },
    {
      {-0.33598870038986206055f}
    },
    {
      {-0.27940231561660766602f}
    },
    {
      {0.27235329151153564453f}
    },
    {
      {-0.02835815399885177612f}
    },
    {
      {-0.14053399860858917236f}
    },
    {
      {-0.21651469171047210693f}
    }
  },
  {
    {
      {-0.00317279878072440624f}
    },
    {
      {-0.23556232452392578125f}
    },
    {
      {-0.32681655883789062500f}
    },
    {
      {-0.15637873113155364990f}
    },
    {
      {-0.09753317385911941528f}
    },
    {
      {-0.03460234403610229492f}
    },
    {
      {-0.30675420165061950684f}
    },
    {
      {-0.31856900453567504883f}
    }
  },
  {
    {
      {-0.34588861465454101562f}
    },
    {
      {0.05962615460157394409f}
    },
    {
      {-0.20666064321994781494f}
    },
    {
      {0.13883881270885467529f}
    },
    {
      {0.08438152819871902466f}
    },
    {
      {0.34374821186065673828f}
    },
    {
      {0.10440729558467864990f}
    },
    {
      {0.20274834334850311279f}
    }
  },
  {
    {
      {-0.22468186914920806885f}
    },
    {
      {-0.01383307483047246933f}
    },
    {
      {-0.31870695948600769043f}
    },
    {
      {-0.31342449784278869629f}
    },
    {
      {-0.08704397827386856079f}
    },
    {
      {-0.01725487597286701202f}
    },
    {
      {0.04971178993582725525f}
    },
    {
      {0.29257771372795104980f}
    }
  },
  {
    {
      {0.31976562738418579102f}
    },
    {
      {-0.17448315024375915527f}
    },
    {
      {0.28152319788932800293f}
    },
    {
      {0.12876826524734497070f}
    },
    {
      {-0.29778629541397094727f}
    },
    {
      {-0.23377655446529388428f}
    },
    {
      {0.03768967092037200928f}
    },
    {
      {-0.33904486894607543945f}
    }
  },
  {
    {
      {-0.31338223814964294434f}
    },
    {
      {0.27061370015144348145f}
    },
    {
      {0.07474807649850845337f}
    },
    {
      {0.28945776820182800293f}
    },
    {
      {-0.04204059392213821411f}
    },
    {
      {0.32371252775192260742f}
    },
    {
      {-0.07168388366699218750f}
    },
    {
      {-0.30276435613632202148f}
    }
  },
  {
    {
      {0.03043674118816852570f}
    },
    {
      {0.27896326780319213867f}
    },
    {
      {-0.12611918151378631592f}
    },
    {
      {0.13662400841712951660f}
    },
    {
      {0.29776945710182189941f}
    },
    {
      {-0.08494090288877487183f}
    },
    {
      {-0.34523466229438781738f}
    },
    {
      {0.20747195184230804443f}
    }
  }
};
static const float tensor_onnx__Conv_79[8] = 
{0.00000000000000000000f, 0.00000000000000000000f, 0.00000000000000000000f, 0.00000000000000000000f, 0.00000000000000000000f, 0.00000000000000000000f, 0.00000000000000000000f, 0.00000000000000000000f};
static const float tensor_onnx__Conv_81[8][4][1] = 
{
  {
    {0.10470570623874664307f},
    {-0.03559279441833496094f},
    {-0.34908613562583923340f},
    {0.13071775436401367188f}
  },
  {
    {-0.32620665431022644043f},
    {-0.19387309253215789795f},
    {0.37341275811195373535f},
    {0.12442073225975036621f}
  },
  {
    {0.33054441213607788086f},
    {0.00542739313095808029f},
    {0.23555673658847808838f},
    {0.46086135506629943848f}
  },
  {
    {-0.25112295150756835938f},
    {0.11387451738119125366f},
    {-0.24613364040851593018f},
    {0.24729599058628082275f}
  },
  {
    {0.00253950268961489201f},
    {0.42006126046180725098f},
    {0.37194669246673583984f},
    {-0.46155631542205810547f}
  },
  {
    {-0.14055331051349639893f},
    {-0.02673445641994476318f},
    {0.00950963515788316727f},
    {0.17151305079460144043f}
  },
  {
    {0.45899519324302673340f},
    {-0.25839194655418395996f},
    {0.32472243905067443848f},
    {-0.11092696338891983032f}
  },
  {
    {0.27536243200302124023f},
    {-0.37990677356719970703f},
    {-0.39549791812896728516f},
    {-0.41183671355247497559f}
  }
};
static const int64_t tensor__Where_output_0_static[4] = 
{8, 4, 10, 3};
static const float tensor__Constant_output_0 = 
-2.00000000000000000000f;
static const float tensor__Constant_1_output_0 = 
2.00000000000000000000f;
static const int64_t tensor_onnx__ReduceSum_22[1] = 
{1};
static const int64_t tensor__Constant_2_output_0[1] = 
{4};
static const int64_t tensor__Constant_3_output_0[1] = 
{2};
static const int64_t tensor__Constant_4_output_0[1] = 
{1};
static const int64_t tensor__Constant_5_output_0[1] = 
{9223372036854775807};
static const int64_t tensor__Constant_6_output_0[1] = 
{1};
static const int64_t tensor__Constant_7_output_0[8][1][1] = 
{
  {
    {0}
  },
  {
    {10}
  },
  {
    {20}
  },
  {
    {30}
  },
  {
    {40}
  },
  {
    {50}
  },
  {
    {60}
  },
  {
    {70}
  }
};
static const int64_t tensor__Constant_8_output_0[1] = 
{-1};
static const int64_t tensor__Constant_9_output_0[2] = 
{4, -1};
static const int64_t tensor__Constant_10_output_0[4] = 
{4, 8, 10, 3};
static const int64_t tensor__Constant_11_output_0[1] = 
{-1};
static const int64_t tensor__Constant_12_output_0[1] = 
{4};
static const int64_t tensor__Constant_13_output_0 = 
-1;
static const int64_t tensor__Constant_14_output_0[4] = 
{8, 4, 10, 3};
static const int64_t tensor__Constant_15_output_0[4] = 
{8, 4, 10, 3};
static const int64_t tensor__Constant_16_output_0[1] = 
{-1};
union tensor_union_0 {
float tensor_onnx__Conv_82[8];
float tensor__Add_1_output_0[8][8][10];
};
static union tensor_union_0 tu0;

union tensor_union_1 {
float tensor__Transpose_output_0[8][10][3];
float tensor__Mul_output_0[8][10][10];
float tensor__Transpose_1_output_0[8][10][1];
float tensor__TopK_output_0[8][10][4];
int64_t tensor__Slice_output_0[8][10][3];
int64_t tensor__Reshape_output_0[240];
float tensor__Reshape_2_output_0[4][8][10][3];
float tensor__Unsqueeze_output_0[8][4][10][1];
float tensor__Sub_2_output_0[8][4][10][3];
float tensor__convs_0_Conv_output_0[8][8][10][3];
float tensor__ReduceMean_output_0[8][8][10];
};
static union tensor_union_1 tu1;

union tensor_union_2 {
float tensor__MatMul_output_0[8][10][10];
float tensor__Pow_output_0[8][3][10];
float tensor__Neg_output_0[8][1][10];
float tensor__Sub_1_output_0[8][10][10];
int64_t tensor__Add_output_0[8][10][3];
float tensor__Transpose_2_output_0[4][8][10];
float tensor__Gather_output_0[4][240];
float tensor__Transpose_3_output_0[8][4][10][3];
float tensor__Concat_output_0[8][8][10][3];
float tensor__acts_0_Relu_output_0[8][8][10][3];
float tensor__sc_Conv_output_0[8][8][10];
};
static union tensor_union_2 tu2;

union tensor_union_3 {
float tensor__ReduceSum_output_0[8][1][10];
int64_t tensor__TopK_output_1[8][10][4];
float tensor__Reshape_1_output_0[4][80];
int64_t tensor__ConstantOfShape_output_0[4];
bool tensor__Equal_output_0[4];
float tensor__Expand_output_0[8][4][10][3];
};
static union tensor_union_3 tu3;

union tensor_union_4 {
float tensor__Sub_output_0[8][10][10];
int64_t tensor__Mul_1_output_0[4];
};
static union tensor_union_4 tu4;


/*
 * Operand:           Identity
 * Name in ONNX file: Identity_18
 */
FUNC_PREFIX void node_Identity_18( const float input[8], float output[8] )
{
	/* Identity */
	const float* input_ptr = (const float*) input;
	float* output_ptr = (float*) output;
	for (unsigned i = 0; i < 8; i++)
		output_ptr[i] = input_ptr[i];
}

/*
 * Operand:           Transpose
 * Name in ONNX file: /Transpose
 */
FUNC_PREFIX void node__Transpose( const float input[8][3][10], float output[8][10][3] )
{
	/* Transpose
	 * perm = 0 2 1 
	 */
	for( uint32_t i0=0; i0<8; i0++ ) {
	for( uint32_t i1=0; i1<3; i1++ ) {
	for( uint32_t i2=0; i2<10; i2++ ) {
		output[i0][i2][i1] = input[i0][i1][i2];
	}
	}
	}
}

/*
 * Operand:           MatMul
 * Name in ONNX file: /MatMul
 */
FUNC_PREFIX void node__MatMul( const float A[8][10][3], const float B[8][3][10], float Y[8][10][10] )
{
	/* MatMul */
	for (unsigned i0=0; i0<8; i0++)
	{
		for (unsigned i = 0; i < 10; i++)
		for (unsigned j = 0; j < 10; j++)
		{
			Y[i0][i][j] = 0;
			for (unsigned k = 0; k < 3; k++)
				Y[i0][i][j] += A[i0][i][k] * B[i0][k][j];
		}
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant
 */
FUNC_PREFIX void node__Constant( const float *output )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Mul
 * Name in ONNX file: /Mul
 */
FUNC_PREFIX void node__Mul( const float A[8][10][10], const float *B, float C[8][10][10] )
{
	/* Mul
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<8; i0++)
	for (unsigned i1=0; i1<10; i1++)
	for (unsigned i2=0; i2<10; i2++)
	{
		C[i0][i1][i2] = A[i0][i1][i2]**B;;
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_1
 */
FUNC_PREFIX void node__Constant_1( const float *output )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Pow
 * Name in ONNX file: /Pow
 */
FUNC_PREFIX void node__Pow( const float A[8][3][10], const float *B, float C[8][3][10] )
{
	/* Pow
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<8; i0++)
	for (unsigned i1=0; i1<3; i1++)
	for (unsigned i2=0; i2<10; i2++)
	{
		C[i0][i1][i2] = powf(A[i0][i1][i2],*B);;
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: Constant_25
 */
FUNC_PREFIX void node_Constant_25( const int64_t output[1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           ReduceSum
 * Name in ONNX file: /ReduceSum
 */
FUNC_PREFIX void node__ReduceSum( const float x[8][3][10], const int64_t axes[1], float y[8][1][10] )
{
	/* ReduceSum */
	/* keepdims: 1 */
	/* axes: (	1	) */
	for (unsigned i0 = 0; i0<8; i0++) {
		for (unsigned i1 = 0; i1<1; i1++) {
			for (unsigned i2 = 0; i2<10; i2++) {
				y[i0][i1][i2] = 0.0;
			}
		}
	}
	for (unsigned i0 = 0; i0<8; i0++) {
		for (unsigned i1 = 0; i1<3; i1++) {
			for (unsigned i2 = 0; i2<10; i2++) {
				y[i0][0][i2]+=x[i0][i1][i2];
			}
		}
	}
}

/*
 * Operand:           Neg
 * Name in ONNX file: /Neg
 */
FUNC_PREFIX void node__Neg( const float X[8][1][10], float Y[8][1][10] )
{
	/* Neg
	   Implemented with Elementwise template.
	   alpha = 0.00000000000000000000
	   beta = 0.00000000000000000000
	*/
	for (unsigned i0=0; i0<8; i0++) {
	for (unsigned i1=0; i1<1; i1++) {
	for (unsigned i2=0; i2<10; i2++) {
		Y[i0][i1][i2] =  -X[i0][i1][i2];
	}
	}
	}
}

/*
 * Operand:           Sub
 * Name in ONNX file: /Sub
 */
FUNC_PREFIX void node__Sub( const float A[8][1][10], const float B[8][10][10], float C[8][10][10] )
{
	/* Sub
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<8; i0++)
	for (unsigned i1=0; i1<10; i1++)
	for (unsigned i2=0; i2<10; i2++)
	{
		C[i0][i1][i2] = A[i0][0][i2]-B[i0][i1][i2];;
	}
}

/*
 * Operand:           Transpose
 * Name in ONNX file: /Transpose_1
 */
FUNC_PREFIX void node__Transpose_1( const float input[8][1][10], float output[8][10][1] )
{
	/* Transpose
	 * perm = 0 2 1 
	 */
	for( uint32_t i0=0; i0<8; i0++ ) {
	for( uint32_t i1=0; i1<1; i1++ ) {
	for( uint32_t i2=0; i2<10; i2++ ) {
		output[i0][i2][i1] = input[i0][i1][i2];
	}
	}
	}
}

/*
 * Operand:           Sub
 * Name in ONNX file: /Sub_1
 */
FUNC_PREFIX void node__Sub_1( const float A[8][10][10], const float B[8][10][1], float C[8][10][10] )
{
	/* Sub
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<8; i0++)
	for (unsigned i1=0; i1<10; i1++)
	for (unsigned i2=0; i2<10; i2++)
	{
		C[i0][i1][i2] = A[i0][i1][i2]-B[i0][i1][0];;
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_2
 */
FUNC_PREFIX void node__Constant_2( const int64_t output[1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           TopK
 * Name in ONNX file: /TopK
 */
FUNC_PREFIX void node__TopK( const float X[8][10][10], const int64_t K[1], float Values[8][10][4], int64_t Indices[8][10][4] )
{
	/* TopK with largest=1, sorted=1, other options not currently supported */
	for (int i0 = 0; i0 < 8; i0++) {
		for (int i1 = 0; i1 < 10; i1++) {
			for (int k = 0; k < 4; k++) {
				Values[i0][i1][k] = -1e10;
				Indices[i0][i1][k] = -1;
			}
			for (int k = 0; k < 10; k++) {
				for (int d = 0; d < 4; d++) {
					if (X[i0][i1][k] > Values[i0][i1][d]) {
						for (int s = 3; s > d; s--) {
							Values[i0][i1][s] = Values[i0][i1][s - 1];
							Indices[i0][i1][s] = Indices[i0][i1][s - 1];
						}
						Values[i0][i1][d] = X[i0][i1][k];
						Indices[i0][i1][d] = k;
						break;
					}
				}
			}
		}
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_3
 */
FUNC_PREFIX void node__Constant_3( const int64_t output[1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_4
 */
FUNC_PREFIX void node__Constant_4( const int64_t output[1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_5
 */
FUNC_PREFIX void node__Constant_5( const int64_t output[1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_6
 */
FUNC_PREFIX void node__Constant_6( const int64_t output[1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Slice
 * Name in ONNX file: /Slice
 */
FUNC_PREFIX void node__Slice( const int64_t data[8][10][4], const int64_t starts[1], const int64_t ends[1], const int64_t axes[1], const int64_t steps[1], int64_t output[8][10][3] )
{
	for (unsigned i0=0, o0=0; o0<8; i0+=1, o0++) {
	for (unsigned i1=0, o1=0; o1<10; i1+=1, o1++) {
	for (unsigned i2=1, o2=0; o2<3; i2+=1, o2++) {
		output[o0][o1][o2] = data[i0][i1][i2];
	}
	}
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_7
 */
FUNC_PREFIX void node__Constant_7( const int64_t output[8][1][1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Add
 * Name in ONNX file: /Add
 */
FUNC_PREFIX void node__Add( const int64_t A[8][10][3], const int64_t B[8][1][1], int64_t C[8][10][3] )
{
	/* Add
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<8; i0++)
	for (unsigned i1=0; i1<10; i1++)
	for (unsigned i2=0; i2<3; i2++)
	{
		C[i0][i1][i2] = A[i0][i1][i2]+B[i0][0][0];;
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_8
 */
FUNC_PREFIX void node__Constant_8( const int64_t output[1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Reshape
 * Name in ONNX file: /Reshape
 */
FUNC_PREFIX void node__Reshape( const int64_t data[8][10][3], const int64_t shape[1], int64_t reshaped[240] )
{
	/*Reshape*/
	int64_t *data_ptr = (int64_t*)data;
	int64_t *reshaped_ptr = (int64_t*)reshaped;
	uint32_t i;
	for( i=0; i<240; i++ )
		reshaped_ptr[i] = data_ptr[i];

}

/*
 * Operand:           Transpose
 * Name in ONNX file: /Transpose_2
 */
FUNC_PREFIX void node__Transpose_2( const float input[8][4][10], float output[4][8][10] )
{
	/* Transpose
	 * perm = 1 0 2 
	 */
	for( uint32_t i0=0; i0<8; i0++ ) {
	for( uint32_t i1=0; i1<4; i1++ ) {
	for( uint32_t i2=0; i2<10; i2++ ) {
		output[i1][i0][i2] = input[i0][i1][i2];
	}
	}
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_9
 */
FUNC_PREFIX void node__Constant_9( const int64_t output[2] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Reshape
 * Name in ONNX file: /Reshape_1
 */
FUNC_PREFIX void node__Reshape_1( const float data[4][8][10], const int64_t shape[2], float reshaped[4][80] )
{
	/*Reshape*/
	float *data_ptr = (float*)data;
	float *reshaped_ptr = (float*)reshaped;
	uint32_t i;
	for( i=0; i<320; i++ )
		reshaped_ptr[i] = data_ptr[i];

}

/*
 * Operand:           Gather
 * Name in ONNX file: /Gather
 */
FUNC_PREFIX void node__Gather( const float X[4][80], const int64_t indices[240], float Y[4][240] )
{
	/* Gather
	   axis = 1
	 */
	for (unsigned i0=0; i0<4; i0++)
	for (unsigned i1=0; i1<240; i1++)
	{
		int32_t idx = indices[i1];
		idx = idx < 0 ? 80+idx : idx;
		Y[i0][i1] = X[i0][idx];
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_10
 */
FUNC_PREFIX void node__Constant_10( const int64_t output[4] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Reshape
 * Name in ONNX file: /Reshape_2
 */
FUNC_PREFIX void node__Reshape_2( const float data[4][240], const int64_t shape[4], float reshaped[4][8][10][3] )
{
	/*Reshape*/
	float *data_ptr = (float*)data;
	float *reshaped_ptr = (float*)reshaped;
	uint32_t i;
	for( i=0; i<960; i++ )
		reshaped_ptr[i] = data_ptr[i];

}

/*
 * Operand:           Transpose
 * Name in ONNX file: /Transpose_3
 */
FUNC_PREFIX void node__Transpose_3( const float input[4][8][10][3], float output[8][4][10][3] )
{
	/* Transpose
	 * perm = 1 0 2 3 
	 */
	for( uint32_t i0=0; i0<4; i0++ ) {
	for( uint32_t i1=0; i1<8; i1++ ) {
	for( uint32_t i2=0; i2<10; i2++ ) {
	for( uint32_t i3=0; i3<3; i3++ ) {
		output[i1][i0][i2][i3] = input[i0][i1][i2][i3];
	}
	}
	}
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_11
 */
FUNC_PREFIX void node__Constant_11( const int64_t output[1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Unsqueeze
 * Name in ONNX file: /Unsqueeze
 */
FUNC_PREFIX void node__Unsqueeze( const float input[8][4][10], const int64_t axes_tensor[1], float output[8][4][10][1] )
{
	/* Unsqueeze */
	float *data = (float*)input;
	float *expanded= (float*)output;
	for( uint32_t i=0; i<320; i++ )
		expanded[i] = data[i];

}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_12
 */
FUNC_PREFIX void node__Constant_12( const int64_t output[1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           ConstantOfShape
 * Name in ONNX file: /ConstantOfShape
 */
FUNC_PREFIX void node__ConstantOfShape( const int64_t input[1], int64_t output[4] )
{
	/* ConstantOfShape */
	int64_t *dst = (int64_t*)output;
	for( unsigned i=0; i< 4; i++)
		dst[i] = 1;
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_13
 */
FUNC_PREFIX void node__Constant_13( const int64_t *output )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Mul
 * Name in ONNX file: /Mul_1
 */
FUNC_PREFIX void node__Mul_1( const int64_t A[4], const int64_t *B, int64_t C[4] )
{
	/* Mul
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<4; i0++)
	{
		C[i0] = A[i0]**B;;
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_14
 */
FUNC_PREFIX void node__Constant_14( const int64_t output[4] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Equal
 * Name in ONNX file: /Equal
 */
FUNC_PREFIX void node__Equal( const int64_t A[4], const int64_t B[4], bool C[4] )
{
	/* Equal
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<4; i0++)
	{
		C[i0] = A[i0]==B[i0];;
	}
}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_15
 */
FUNC_PREFIX void node__Constant_15( const int64_t output[4] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           Expand
 * Name in ONNX file: /Expand
 */
FUNC_PREFIX void node__Expand( const float input[8][4][10][1], const int64_t shape[4], float output[8][4][10][3] )
{
	/* Expand */
	for( uint32_t o0=0; o0<8; o0++) {
	for( uint32_t o1=0; o1<4; o1++) {
	for( uint32_t o2=0; o2<10; o2++) {
	for( uint32_t o3=0; o3<3; o3++) {
		output[o0][o1][o2][o3] = input[o0][o1][o2][0];
	}
	}
	}
	}
}

/*
 * Operand:           Sub
 * Name in ONNX file: /Sub_2
 */
FUNC_PREFIX void node__Sub_2( const float A[8][4][10][3], const float B[8][4][10][3], float C[8][4][10][3] )
{
	/* Sub
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<8; i0++)
	for (unsigned i1=0; i1<4; i1++)
	for (unsigned i2=0; i2<10; i2++)
	for (unsigned i3=0; i3<3; i3++)
	{
		C[i0][i1][i2][i3] = A[i0][i1][i2][i3]-B[i0][i1][i2][i3];;
	}
}

/*
 * Operand:           Concat
 * Name in ONNX file: /Concat
 */
FUNC_PREFIX void node__Concat( const float input_0[8][4][10][3], const float input_1[8][4][10][3], float output[8][8][10][3] )
{
	/* Concat */
	int64_t outputOffset;
	outputOffset = 0;
	for (int64_t i = 0, j = 0; i < 960; i++) {
		*((float*)output + (outputOffset + i)) = *((float*)input_0 + i);
		if (++j == 120) {
			outputOffset += (120);
			j = 0;
		}
	}
	outputOffset = 120;
	for (int64_t i = 0, j = 0; i < 960; i++) {
		*((float*)output + (outputOffset + i)) = *((float*)input_1 + i);
		if (++j == 120) {
			outputOffset += (120);
			j = 0;
		}
	}
}

/*
 * Operand:           Conv
 * Name in ONNX file: /convs.0/Conv
 */
FUNC_PREFIX void node__convs_0_Conv( const float x[8][8][10][3], const float w[8][8][1][1], const float bias[8], float y[8][8][10][3] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 1 1 
	 * pads: 0 0 0 0 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<8; b++ ) {
	for( uint32_t m=0; m<8; m++) {
		for( int32_t o0=0, i0=0; o0<10; o0++, i0+=1) {
		for( int32_t o1=0, i1=0; o1<3; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<8; c++ ) {
			for( uint32_t k0=0; k0<1; k0++ ) {
			for( uint32_t k1=0; k1<1; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=10) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=3) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] * w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: /acts.0/Relu
 */
FUNC_PREFIX void node__acts_0_Relu( const float X[8][8][10][3], float Y[8][8][10][3] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<1920; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Constant
 * Name in ONNX file: /Constant_16
 */
FUNC_PREFIX void node__Constant_16( const int64_t output[1] )
{
	/* Constant */
	/* The output is generated as a global tensor */
	(void)output;
}

/*
 * Operand:           ReduceMean
 * Name in ONNX file: /ReduceMean
 */
FUNC_PREFIX void node__ReduceMean( const float x[8][8][10][3], const int64_t axes[1], float y[8][8][10] )
{
	/* ReduceMean */
	/* keepdims: 0 */
	/* axes: (	-1	) */
	for (unsigned i0 = 0; i0<8; i0++) {
		for (unsigned i1 = 0; i1<8; i1++) {
			for (unsigned i2 = 0; i2<10; i2++) {
				y[i0][i1][i2] = 0.0;
			}
		}
	}
	for (unsigned i0 = 0; i0<8; i0++) {
		for (unsigned i1 = 0; i1<8; i1++) {
			for (unsigned i2 = 0; i2<10; i2++) {
				for (unsigned i3 = 0; i3<3; i3++) {
					y[i0][i1][i2]+=x[i0][i1][i2][i3];
				}
			}
		}
	}
	/* ReduceMean: Divide by the number of elements (reduced axes) */
	for (unsigned i0 = 0; i0<8; i0++) {
		for (unsigned i1 = 0; i1<8; i1++) {
			for (unsigned i2 = 0; i2<10; i2++) {
				y[i0][i1][i2] /= 3;
			}
		}
	}
}

/*
 * Operand:           Conv
 * Name in ONNX file: /sc/Conv
 */
FUNC_PREFIX void node__sc_Conv( const float x[8][4][10], const float w[8][4][1], const float bias[8], float y[8][8][10] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 
	 * group: 1
	 * kernel_shape: 1 
	 * pads: 0 0 
	 * strides: 1 
	 */
	for( uint32_t b=0; b<8; b++ ) {
	for( uint32_t m=0; m<8; m++) {
		for( int32_t o0=0, i0=0; o0<10; o0++, i0+=1) {
			y[b][m][o0] = bias[m];
			for( int32_t c=0; c<4; c++ ) {
			for( uint32_t k0=0; k0<1; k0++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=10) continue;
				y[b][m][o0] += x[b][c][ii0] * w[m][c][k0];
			} /* k */
			} /* c */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Add
 * Name in ONNX file: /Add_1
 */
FUNC_PREFIX void node__Add_1( const float A[8][8][10], const float B[8][8][10], float C[8][8][10] )
{
	/* Add
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<8; i0++)
	for (unsigned i1=0; i1<8; i1++)
	for (unsigned i2=0; i2<10; i2++)
	{
		C[i0][i1][i2] = A[i0][i1][i2]+B[i0][i1][i2];;
	}
}

/*
 * Operand:           Relu
 * Name in ONNX file: /sc_act/Relu
 */
FUNC_PREFIX void node__sc_act_Relu( const float X[8][8][10], float Y[8][8][10] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<640; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}


void entry(const float tensor_pf_points[8][3][10], const float tensor_pf_features[8][4][10], float tensor_output[8][8][10]){
	node_Identity_18( tensor_onnx__Conv_79, tu0.tensor_onnx__Conv_82);
	node__Transpose( tensor_pf_points, tu1.tensor__Transpose_output_0);
	node__MatMul( tu1.tensor__Transpose_output_0, tensor_pf_points, tu2.tensor__MatMul_output_0);
	node__Constant( &tensor__Constant_output_0);
	node__Mul( tu2.tensor__MatMul_output_0, &tensor__Constant_output_0, tu1.tensor__Mul_output_0);
	node__Constant_1( &tensor__Constant_1_output_0);
	node__Pow( tensor_pf_points, &tensor__Constant_1_output_0, tu2.tensor__Pow_output_0);
	node_Constant_25( tensor_onnx__ReduceSum_22);
	node__ReduceSum( tu2.tensor__Pow_output_0, tensor_onnx__ReduceSum_22, tu3.tensor__ReduceSum_output_0);
	node__Neg( tu3.tensor__ReduceSum_output_0, tu2.tensor__Neg_output_0);
	node__Sub( tu2.tensor__Neg_output_0, tu1.tensor__Mul_output_0, tu4.tensor__Sub_output_0);
	node__Transpose_1( tu3.tensor__ReduceSum_output_0, tu1.tensor__Transpose_1_output_0);
	node__Sub_1( tu4.tensor__Sub_output_0, tu1.tensor__Transpose_1_output_0, tu2.tensor__Sub_1_output_0);
	node__Constant_2( tensor__Constant_2_output_0);
	node__TopK( tu2.tensor__Sub_1_output_0, tensor__Constant_2_output_0, tu1.tensor__TopK_output_0, tu3.tensor__TopK_output_1);
	node__Constant_3( tensor__Constant_3_output_0);
	node__Constant_4( tensor__Constant_4_output_0);
	node__Constant_5( tensor__Constant_5_output_0);
	node__Constant_6( tensor__Constant_6_output_0);
	node__Slice( tu3.tensor__TopK_output_1, tensor__Constant_4_output_0, tensor__Constant_5_output_0, tensor__Constant_3_output_0, tensor__Constant_6_output_0, tu1.tensor__Slice_output_0);
	node__Constant_7( tensor__Constant_7_output_0);
	node__Add( tu1.tensor__Slice_output_0, tensor__Constant_7_output_0, tu2.tensor__Add_output_0);
	node__Constant_8( tensor__Constant_8_output_0);
	node__Reshape( tu2.tensor__Add_output_0, tensor__Constant_8_output_0, tu1.tensor__Reshape_output_0);
	node__Transpose_2( tensor_pf_features, tu2.tensor__Transpose_2_output_0);
	node__Constant_9( tensor__Constant_9_output_0);
	node__Reshape_1( tu2.tensor__Transpose_2_output_0, tensor__Constant_9_output_0, tu3.tensor__Reshape_1_output_0);
	node__Gather( tu3.tensor__Reshape_1_output_0, tu1.tensor__Reshape_output_0, tu2.tensor__Gather_output_0);
	node__Constant_10( tensor__Constant_10_output_0);
	node__Reshape_2( tu2.tensor__Gather_output_0, tensor__Constant_10_output_0, tu1.tensor__Reshape_2_output_0);
	node__Transpose_3( tu1.tensor__Reshape_2_output_0, tu2.tensor__Transpose_3_output_0);
	node__Constant_11( tensor__Constant_11_output_0);
	node__Unsqueeze( tensor_pf_features, tensor__Constant_11_output_0, tu1.tensor__Unsqueeze_output_0);
	node__Constant_12( tensor__Constant_12_output_0);
	node__ConstantOfShape( tensor__Constant_12_output_0, tu3.tensor__ConstantOfShape_output_0);
	node__Constant_13( &tensor__Constant_13_output_0);
	node__Mul_1( tu3.tensor__ConstantOfShape_output_0, &tensor__Constant_13_output_0, tu4.tensor__Mul_1_output_0);
	node__Constant_14( tensor__Constant_14_output_0);
	node__Equal( tensor__Constant_14_output_0, tu4.tensor__Mul_1_output_0, tu3.tensor__Equal_output_0);
	node__Constant_15( tensor__Constant_15_output_0);
	node__Expand( tu1.tensor__Unsqueeze_output_0, tensor__Where_output_0_static, tu3.tensor__Expand_output_0);
	node__Sub_2( tu2.tensor__Transpose_3_output_0, tu3.tensor__Expand_output_0, tu1.tensor__Sub_2_output_0);
	node__Concat( tu3.tensor__Expand_output_0, tu1.tensor__Sub_2_output_0, tu2.tensor__Concat_output_0);
	node__convs_0_Conv( tu2.tensor__Concat_output_0, tensor_onnx__Conv_78, tensor_onnx__Conv_79, tu1.tensor__convs_0_Conv_output_0);
	node__acts_0_Relu( tu1.tensor__convs_0_Conv_output_0, tu2.tensor__acts_0_Relu_output_0);
	node__Constant_16( tensor__Constant_16_output_0);
	node__ReduceMean( tu2.tensor__acts_0_Relu_output_0, tensor__Constant_16_output_0, tu1.tensor__ReduceMean_output_0);
	node__sc_Conv( tensor_pf_features, tensor_onnx__Conv_81, tu0.tensor_onnx__Conv_82, tu2.tensor__sc_Conv_output_0);
	node__Add_1( tu2.tensor__sc_Conv_output_0, tu1.tensor__ReduceMean_output_0, tu0.tensor__Add_1_output_0);
	node__sc_act_Relu( tu0.tensor__Add_1_output_0, tensor_output);
}
